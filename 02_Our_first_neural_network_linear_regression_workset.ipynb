{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeeterNeumann/DeepLearning/blob/main/02_Our_first_neural_network_linear_regression_workset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UVZlK6AjphyK"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zhrag2TVqUut"
      },
      "outputs": [],
      "source": [
        "def seed_all(seed=42):\n",
        "    \"\"\"\n",
        "    Sets the numpy and torch random seed.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.random.seed = seed\n",
        "\n",
        "seed_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i5JbcrJp8Ng"
      },
      "outputs": [],
      "source": [
        "# Create some X data\n",
        "X = np.random.uniform(0, 10, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWXuFmJPqglE"
      },
      "outputs": [],
      "source": [
        "# Define the slope (m), bias (b), and some noise we want to add to X to make y\n",
        "m = 3\n",
        "b = 1.8\n",
        "noise = np.random.normal(scale=3, size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaATIViIcyK_"
      },
      "source": [
        "# Exercise 2.1\n",
        "\n",
        "Based on your knowledge of linear regression, please use the variables `X`, `m`, `b`, and `noise` to generate a new variable `y`.\n",
        "\n",
        "<!-- startquestion -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCGl3ng2dSRw"
      },
      "outputs": [],
      "source": [
        "y = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R0i1APRq0SW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('y')\n",
        "ax.scatter(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mMYN-sAdqc1"
      },
      "source": [
        "# Exercise 2.2\n",
        "\n",
        "Please complete the `mse` function below.\n",
        "There is a sanity check implemented that shoudl let you know if you've implemented the function correctly.\n",
        "\n",
        "<!-- startquestion -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "duBXRwuidjI0"
      },
      "outputs": [],
      "source": [
        "# Define MSE\n",
        "def mse(predictions:torch.Tensor, actuals:torch.Tensor) -> torch.Tensor:\n",
        "    yhats = predictions\n",
        "    ys = actuals\n",
        "    mse = ((yhats - ys)**2).mean()\n",
        "    return mse\n",
        "    raise NotImplementedError(\"Implement MSE, then remove this line\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YXVJjYjQYS-r"
      },
      "outputs": [],
      "source": [
        "ys = torch.tensor([1,2,3])\n",
        "yhats = torch.tensor([1.1, 2.1, 3.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "aqjXsI8dr4ba",
        "outputId": "9c045c04-d050-4a3b-a304-0b188b6bb277"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-278485530.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "assert mse(ys, yhats) == 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXN3G-Fq4m9a",
        "outputId": "b505e0f2-4c79-496d-da82-35aa72c9f960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0100)\n",
            "0.009999989\n"
          ]
        }
      ],
      "source": [
        "print(mse(ys, yhats))\n",
        "print(mse(ys, yhats).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T4dfoBM14tOq"
      },
      "outputs": [],
      "source": [
        "assert torch.allclose(mse(ys, yhats), torch.tensor(0.01))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmtAEb1_YOo6",
        "outputId": "6fdb2a8c-b1cc-4845-c226-8d8ed58197f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0100)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Now that we've defined MSE, let's just use Torch's.\n",
        "mse_loss = nn.MSELoss()\n",
        "mse_loss(ys, yhats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HROGoKWjY0g1",
        "outputId": "1ed2a23a-74b6-4754-9d4e-2f0f13e86884"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0100)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# We can also use the functional API to calculate MSE\n",
        "F.mse_loss(ys, yhats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_i03QSwHZQFj"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yIFPBzglI7u"
      },
      "source": [
        "# Exercise 2.3\n",
        "\n",
        "In the exercise below, use the notebook to fit a `LinearRegression` model.\n",
        "We will inspect the parameters (the coefficient and intercept), and calculate the mean squared error.\n",
        "In the cells below, complete the exercise to determine the coefficient and intercept learned by a `LinearRegression`.\n",
        "\n",
        "<!-- startquestion -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1rMNqDneidD"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "lr = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp5rB1BWZnyE"
      },
      "outputs": [],
      "source": [
        "# Display the slope and intercept\n",
        "lr.coef_, lr.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PAk2Xt3cAEy"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean squared error\n",
        "mean_squared_error(y, lr.predict(X.reshape(-1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eIhs0pjZyem"
      },
      "outputs": [],
      "source": [
        "# Plot our line of best fit\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('y')\n",
        "ax.scatter(X, y)\n",
        "_x = np.arange(0, 10)\n",
        "_y = _x * lr.coef_[0] + lr.intercept_\n",
        "ax.plot(_x, _y, c='red', label=f\"Line of best fit\")\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOJ2WiAyxGyo"
      },
      "outputs": [],
      "source": [
        "# Because we're in torch now, let's just turn X and y into tensors.\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Kd4qcgnnK3"
      },
      "source": [
        "# Exercise 2.4\n",
        "\n",
        "In the exercise below, complete the `forward` method.\n",
        "What is the input `X`?\n",
        "How should that be transformed to the output for a linear regression?\n",
        "\n",
        "<!-- startquestion -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2dfhB3fg1Jn"
      },
      "outputs": [],
      "source": [
        "# Build our linear regression model\n",
        "class LinReg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Randomly initialize 2 parameters, one for our slope and one for our bias.\n",
        "        self.slope = nn.Parameter(torch.rand(1))\n",
        "        self.bias = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, X):\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okgeYEnUtfHx"
      },
      "outputs": [],
      "source": [
        "lr = LinReg()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5peJppY0Tsm"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 300\n",
        "LR = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGi-JIk-0ZP7"
      },
      "outputs": [],
      "source": [
        "slopes = []\n",
        "biases = []\n",
        "losses = []\n",
        "_alphas = []\n",
        "for i in range(N_EPOCHS):\n",
        "    # Make some inferences\n",
        "    yhat = lr(X)\n",
        "    # Measure how bad those guesses were\n",
        "    loss = F.mse_loss(yhat, y)\n",
        "    if i%(N_EPOCHS/10)==0:\n",
        "        print(f\"Epoch {i} Train Loss: {loss:.04f}\")\n",
        "    # Calculate the gradient of all the parameters with respect to the loss\n",
        "    loss.backward()\n",
        "    # Apply the SGD update rule\n",
        "    lr.slope.data.sub_(lr.slope.grad * LR)\n",
        "    lr.bias.data.sub_(lr.bias.grad * LR)\n",
        "    # Zero out the gradients for the next round\n",
        "    lr.slope.grad.zero_()\n",
        "    lr.bias.grad.zero_()\n",
        "\n",
        "    # Record the parameters and losses so we can plot them out later\n",
        "    slopes.append(float(lr.slope.data.detach().numpy()))\n",
        "    biases.append(float(lr.bias.data.detach().numpy()))\n",
        "    losses.append(float(loss.detach().numpy()))\n",
        "    _alphas.append(i/N_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgyFeEBudKpF"
      },
      "outputs": [],
      "source": [
        "lr.slope, lr.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrWoCKIV1z4X"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('y')\n",
        "ax.scatter(X, y)\n",
        "for s, b, a in zip(slopes, biases, _alphas):\n",
        "    _x = np.arange(0, 10)\n",
        "    _y = _x * s + b\n",
        "    ax.plot(_x, _y, alpha=a, c='red', label=f\"Epoch {int(a)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gQXK8bw2NL-"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(14,10))\n",
        "ax.plot(losses)\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss (MSE)')\n",
        "if (losses[-1] > losses[0]) | np.isnan(losses[-1]):\n",
        "    ax.set_title('Diverging - BAD!')\n",
        "else:\n",
        "    ax.set_title('Converging - goood!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMMiW5YRhCQF"
      },
      "source": [
        "# Exercise 2.5\n",
        "\n",
        "Change the learning rate and number of epochs, then re-run the code up to this point, making sure to re-instantiate your model every time.\n",
        "What do you notice?\n",
        "What happens if you make the learning rate too large or too small?\n",
        "\n",
        "<!-- startquestion -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BMXex0Xjlty"
      },
      "outputs": [],
      "source": [
        "# Go back and re-run the code with different hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VVJBDwN6X-_"
      },
      "outputs": [],
      "source": [
        "# Make yet another fake dataset\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=1000, n_features=3, n_informative=2, bias=3, noise=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XHTBONL8APT"
      },
      "outputs": [],
      "source": [
        "# No more bad habits, we need to split our data.\n",
        "X_train, X_valid, y_train, y_valid = (torch.tensor(i).float() for i in train_test_split(X, y, test_size=0.1, random_state=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9o7gG1hqaey"
      },
      "source": [
        "# Exercise 2.6\n",
        "\n",
        "Complete the exercise below to create a few `nn.Parameter`s for our weights and bias.\n",
        "The `weights` parameter should have the same number of elements as `X_train` has columns, and the `bias` parameter should just be a single value.\n",
        "Use `torch.rand` to generate random numbers to populate the parameters.\n",
        "\n",
        "<!-- startquestion -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e79s8POBiRaF"
      },
      "outputs": [],
      "source": [
        "# Let's create some temporary weights and biases and test out our matrix operations before we build our model.\n",
        "# Create a weights parameter with 1 beta per column in X\n",
        "weights = ...\n",
        "# Create our bias parameter\n",
        "bias = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW3sWkn49A9r"
      },
      "outputs": [],
      "source": [
        "# Test out the operation we want to perform in the forward pass\n",
        "torch.matmul(X_train[:10], weights) + bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8bauLhD7N8V"
      },
      "outputs": [],
      "source": [
        "# FYI: @ does the same thing as matmul in this context and is easier\n",
        "X_train[:10]@weights + bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYRcEdGH8aN3"
      },
      "outputs": [],
      "source": [
        "# Sanity check: different implementations of our forward pass are the same\n",
        "assert (X_train@weights + bias == torch.matmul(X_train, weights) + bias).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNzWV8938wjO"
      },
      "outputs": [],
      "source": [
        "# Let's make our model\n",
        "class LinRegMulti(nn.Module):\n",
        "    def __init__(self, n_cols):\n",
        "        super().__init__()\n",
        "        self.n_cols = n_cols\n",
        "\n",
        "        self.weights = nn.Parameter(torch.rand(self.n_cols))\n",
        "        self.bias = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X@self.weights.T + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEzQbpnR8hqO"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 10000\n",
        "LR = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfWaPFjW9dfb"
      },
      "outputs": [],
      "source": [
        "lrm = LinRegMulti(X_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlRs5GF7-CQh"
      },
      "outputs": [],
      "source": [
        "# Instead of updating each parameter individually, let's make an update rule function.\n",
        "def gd_update_rule(parameters, lr):\n",
        "    parameters.data.sub_(parameters.grad * lr)\n",
        "    parameters.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzk5omp6-hEX"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "valid_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GczinipS9hRz"
      },
      "outputs": [],
      "source": [
        "for i in range(N_EPOCHS):\n",
        "    yhat = lrm(X_train)\n",
        "    loss = mse(yhat, y_train)\n",
        "    loss.backward()\n",
        "    for p in lrm.parameters():\n",
        "        gd_update_rule(p, LR)\n",
        "    train_losses.append(loss.detach().numpy())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        yhat = lrm(X_valid)\n",
        "        valid_loss = mse(yhat, y_valid)\n",
        "        valid_losses.append(valid_loss.numpy())\n",
        "\n",
        "    if i%(N_EPOCHS/10) == 0:\n",
        "        print(f\"Epoch {i} Train Loss: {loss:.04f}, Valid Loss: {valid_loss:.04f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoVuHW7a9iau"
      },
      "outputs": [],
      "source": [
        "EPOCHS_TO_SHOW = 2000\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "ax.plot(train_losses[:EPOCHS_TO_SHOW], label='Train', linewidth=3, alpha=0.5)\n",
        "ax.plot(valid_losses[:EPOCHS_TO_SHOW], ls='--', label='Valid')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVxNByIg-mas"
      },
      "outputs": [],
      "source": [
        "lrm.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYNLiUqy-yxx"
      },
      "outputs": [],
      "source": [
        "lrm.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59HrUTC5_YfS"
      },
      "outputs": [],
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.rand((dim_in, dim_out)))\n",
        "        self.bias = nn.Parameter(torch.rand(dim_out))\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        return X@self.weights + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glH-bIYp8JxC"
      },
      "outputs": [],
      "source": [
        "# Let's compare our Linear class with nn.Linear\n",
        "l1 = Linear(3, 5)\n",
        "l2 = nn.Linear(3, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNAS7jI99yGE"
      },
      "outputs": [],
      "source": [
        "l2.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgwbpaGw9wCT"
      },
      "outputs": [],
      "source": [
        "l1.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHTeLzXP_Rzc"
      },
      "outputs": [],
      "source": [
        "# We need to make sure the weights have the same values.\n",
        "# If they don't, we won't be able to compare the output.\n",
        "# I'm not sure why the Linear layer's weights are transposed,\n",
        "# but we'll see it doesn't matter.\n",
        "l1.weights.data.copy_(l2.weight.T)\n",
        "l1.bias.data.copy_(l2.bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZsxCvsZ8c5R"
      },
      "outputs": [],
      "source": [
        "l1(X_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-l2dsw78oyO"
      },
      "outputs": [],
      "source": [
        "l2(X_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJK1Q27O_cdR"
      },
      "outputs": [],
      "source": [
        "assert (l1(X_train[:5]) == l2(X_train[:5])).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRV8GSei7rt_"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        "l1(X_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFOymbqq7wHO"
      },
      "outputs": [],
      "source": [
        "%%timeit\n",
        " l2(X_train[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE7TAbIG2tjD"
      },
      "outputs": [],
      "source": [
        "rng = torch.arange(-5, 5.01, 0.05)\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "ax.plot(rng, F.relu(rng), label='ReLU')\n",
        "ax.plot(rng, torch.tanh(rng), label='tanh')\n",
        "ax.plot(rng, torch.sigmoid(rng), label='sigmoid')\n",
        "ax.plot(rng, F.leaky_relu(rng, negative_slope=0.01), ls='--', label='leaky ReLU')\n",
        "ax.set_ylim(-1.1, 1.1)\n",
        "ax.set_title('Common activation functions')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1zUoNys_9QL"
      },
      "outputs": [],
      "source": [
        "class MultiLayerRegressor(nn.Module):\n",
        "    def __init__(self, dim_in, hidden_dim):\n",
        "        super().__init__()\n",
        "        # self.first_layer = Linear(dim_in, hidden_dim)\n",
        "        self.first_layer = nn.Linear(dim_in, hidden_dim)\n",
        "        # self.second_layer = Linear(hidden_dim, 1)\n",
        "        self.second_layer = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = self.first_layer(X)\n",
        "        # x = relu(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.second_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc0dA6sxIFoe"
      },
      "outputs": [],
      "source": [
        "def multilayer_regressor(in_dim, hidden_dim):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, 1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKLZZbRBAaBD"
      },
      "outputs": [],
      "source": [
        "# mlr = MultiLayerRegressor(3, 4)\n",
        "mlr = multilayer_regressor(3, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oaN64-iAwJ4"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "valid_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt1uzMjZBkWQ"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3\n",
        "N_EPOCHS = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPuw46kBFdty"
      },
      "outputs": [],
      "source": [
        "# Notice that instead of iterating through our parameters and applying\n",
        "# an update rule, we're just using torch's built in SGD optimizer.\n",
        "opt = optim.SGD(mlr.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf91-OmaAepE"
      },
      "outputs": [],
      "source": [
        "for i in range(N_EPOCHS):\n",
        "    yhat = mlr(X_train).squeeze()\n",
        "    loss = F.mse_loss(yhat, y_train)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "    train_losses.append(loss.detach().numpy())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        yhat = mlr(X_valid).squeeze()\n",
        "        valid_loss = F.mse_loss(yhat, y_valid)\n",
        "        valid_losses.append(loss.numpy())\n",
        "\n",
        "    if i%(N_EPOCHS/10) == 0:\n",
        "        print(f\"Epoch {i} Train loss: {loss:.04f}, Valid loss: {valid_loss:.04f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FImQTDQA278"
      },
      "outputs": [],
      "source": [
        "idx=10000\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "ax.plot(train_losses[:idx], label='Train', linewidth=3, alpha=0.5)\n",
        "ax.plot(valid_losses[:idx], ls='--', label='Valid')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXkBM19al6Ge"
      },
      "outputs": [],
      "source": [
        "# Modify the code above to complete the exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-WNsgD3UZlj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "02_Our_first_neural_network_linear_regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "cndl",
      "language": "python",
      "name": "cndl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}